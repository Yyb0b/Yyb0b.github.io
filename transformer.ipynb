{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import numpy as np\n",
    "import random, math\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "# device = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 1\n",
    "setup_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('new_27.csv')\n",
    "\n",
    "max_seq_len1 = 6  # 7 (all-16) 6\n",
    "max_seq_len2 = 5  # 4 (all-16) 5\n",
    "\n",
    "data = data[(data['电共振峰对应位置'].notnull()) & (data['磁共振峰对应位置'].notnull()) & (data['minRL'].notnull())]\n",
    "\n",
    "src_key_padding_mask1 = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data['电共振峰对应位置'].apply(lambda x: [False for i in str(x).split(',')] if x is not np.NaN else []), padding='post', value=True, maxlen=max_seq_len1, dtype=bool\n",
    ")\n",
    "\n",
    "src_key_padding_mask2 = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data['磁共振峰对应位置'].apply(lambda x: [False for i in str(x).split(',')] if x is not np.NaN else []), padding='post', value=True, maxlen=max_seq_len2, dtype=bool\n",
    ")\n",
    "\n",
    "e_features = ['电共振峰对应位置', '电共振峰对应位置_e', '电共振峰对应位置_ee', '电共振峰对应位置_m', '电共振峰对应位置_mm']\n",
    "m_features = ['磁共振峰对应位置', '磁共振峰对应位置_e', '磁共振峰对应位置_ee', '磁共振峰对应位置_m', '磁共振峰对应位置_mm']\n",
    "\n",
    "for col in e_features:\n",
    "    data[col]  = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        data[col].apply(lambda x: [float(i) for i in str(x).split(',')] if x is not np.NaN else []), padding=\"post\", value=np.nan, maxlen=max_seq_len1, dtype=float\n",
    "    ).tolist()\n",
    "\n",
    "for col in m_features:\n",
    "    data[col]  = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        data[col].apply(lambda x: [float(i) for i in str(x).split(',')] if x is not np.NaN else []), padding=\"post\", value=np.nan, maxlen=max_seq_len2, dtype=float\n",
    "    ).tolist()\n",
    "\n",
    "X1 = np.stack(map(lambda i: np.stack(data.loc[i, e_features]), data.index))\n",
    "src_key_padding_mask1 = np.isnan(X1)[:, 0, :]\n",
    "X1[np.isnan(X1)] = 0\n",
    "X2 = np.stack(map(lambda i: np.stack(data.loc[i, m_features]), data.index))\n",
    "src_key_padding_mask2 = np.isnan(X2)[:, 0, :]\n",
    "X2[np.isnan(X2)] = 0\n",
    "Y = data['minRL'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape: (27, 5, 5)\n",
      "src_key_padding_mask1.shape: (27, 5)\n",
      "X2.shape: (27, 5, 5)\n",
      "src_key_padding_mask2.shape: (27, 5)\n",
      "Y.shape: (27, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X1.shape:', X1.shape)\n",
    "print('src_key_padding_mask1.shape:', src_key_padding_mask1.shape)\n",
    "print('X2.shape:', X2.shape)\n",
    "print('src_key_padding_mask2.shape:', src_key_padding_mask2.shape)\n",
    "print('Y.shape:', Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaterialDataset(Dataset):\n",
    "    def __init__(self, X1, X2, Y, src_key_padding_mask1, src_key_padding_mask2):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "        self.src_key_padding_mask1 = src_key_padding_mask1\n",
    "        self.src_key_padding_mask2 = src_key_padding_mask2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X1[index], self.X2[index], self.Y[index], self.src_key_padding_mask1[index], self.src_key_padding_mask2[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        div_term = torch.exp(torch.arange(1, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        # print('pe shape:', pe.shape)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, max_len1: int, max_len2: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.pos_encoder1 = PositionalEncoding(d_model, dropout, max_len=max_len1)\n",
    "        self.transformer_encoder1 = TransformerEncoder(TransformerEncoderLayer(d_model, nhead, d_hid, dropout), nlayers)\n",
    "        \n",
    "        self.pos_encoder2 = PositionalEncoding(d_model, dropout, max_len=max_len2)\n",
    "        self.transformer_encoder2 = TransformerEncoder(TransformerEncoderLayer(d_model, nhead, d_hid, dropout), nlayers)\n",
    "\n",
    "        # self.decoder = nn.Linear(d_model * 5, ntoken) # 相加\n",
    "        self.decoder = nn.Linear(d_model * (max_len1 + max_len2), ntoken) # 拼接\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, X1: Tensor, X2: Tensor, src_key_padding_mask1: Tensor, src_key_padding_mask2: Tensor) -> Tensor:\n",
    "\n",
    "        src = self.pos_encoder1(X1)\n",
    "        output1 = self.transformer_encoder1(src=src, src_key_padding_mask = src_key_padding_mask1)\n",
    "\n",
    "        src = self.pos_encoder2(X2)\n",
    "        output2 = self.transformer_encoder2(src=src, src_key_padding_mask = src_key_padding_mask2)\n",
    "        \n",
    "        output = torch.cat([output1.permute(1, 0, 2).flatten(1), output2.permute(1, 0, 2).flatten(1)], dim=1) # 拼接\n",
    "        # output = output1.permute(1, 0, 2).flatten(1) + output2.permute(1, 0, 2).flatten(1) # 相加\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, device):\n",
    "    model.eval()\n",
    "    score = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x1, x2, y, padding_mask1, padding_mask2) in enumerate(val_loader):\n",
    "            x1 = x1.float().to(device)\n",
    "            x2 = x2.float().to(device)\n",
    "            y = y.float()\n",
    "            padding_mask1 = padding_mask1.float().to(device)\n",
    "            padding_mask2 = padding_mask2.float().to(device)\n",
    "\n",
    "            pre_y = model(x1.permute(2,0,1), x2.permute(2,0,1), padding_mask1, padding_mask2)\n",
    "            pre_y = pre_y.detach().numpy()\n",
    "            # print(pre_y.shape)\n",
    "            score += max(0, r2_score(y, pre_y))\n",
    "            \n",
    "        score /= (i + 1)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************1********************************\n",
      "epoch[0]: loss: 15.001664 score: 0.000000\n",
      "Higher score: 0.000000\n",
      "epoch[1104]: loss: 3.601116 score: 0.001531\n",
      "Higher score: 0.001531\n",
      "epoch[1105]: loss: 3.642489 score: 0.020683\n",
      "Higher score: 0.020683\n",
      "epoch[1106]: loss: 3.435272 score: 0.022794\n",
      "Higher score: 0.022794\n",
      "epoch[1116]: loss: 3.616039 score: 0.023556\n",
      "Higher score: 0.023556\n",
      "epoch[1117]: loss: 3.314700 score: 0.047268\n",
      "Higher score: 0.047268\n",
      "epoch[1118]: loss: 3.508588 score: 0.055319\n",
      "Higher score: 0.055319\n",
      "epoch[1123]: loss: 3.402287 score: 0.055393\n",
      "Higher score: 0.055393\n",
      "epoch[1152]: loss: 3.279778 score: 0.066654\n",
      "Higher score: 0.066654\n",
      "epoch[1153]: loss: 3.662340 score: 0.090920\n",
      "Higher score: 0.090920\n",
      "epoch[1154]: loss: 3.405418 score: 0.099556\n",
      "Higher score: 0.099556\n",
      "epoch[1155]: loss: 3.595795 score: 0.102047\n",
      "Higher score: 0.102047\n",
      "epoch[1158]: loss: 3.108091 score: 0.104625\n",
      "Higher score: 0.104625\n",
      "epoch[1159]: loss: 3.432725 score: 0.112709\n",
      "Higher score: 0.112709\n",
      "epoch[1160]: loss: 3.623890 score: 0.118919\n",
      "Higher score: 0.118919\n",
      "epoch[1325]: loss: 2.925456 score: 0.119646\n",
      "Higher score: 0.119646\n",
      "epoch[1326]: loss: 3.028814 score: 0.143415\n",
      "Higher score: 0.143415\n",
      "epoch[1327]: loss: 3.423503 score: 0.148215\n",
      "Higher score: 0.148215\n",
      "epoch[1328]: loss: 3.103878 score: 0.150896\n",
      "Higher score: 0.150896\n",
      "epoch[1329]: loss: 3.525710 score: 0.152983\n",
      "Higher score: 0.152983\n",
      "epoch[1330]: loss: 3.271575 score: 0.157148\n",
      "Higher score: 0.157148\n",
      "epoch[1331]: loss: 3.808346 score: 0.161213\n",
      "Higher score: 0.161213\n",
      "epoch[1332]: loss: 3.499005 score: 0.161840\n",
      "Higher score: 0.161840\n",
      "epoch[1333]: loss: 3.502193 score: 0.163794\n",
      "Higher score: 0.163794\n",
      "early stopping!!!!\n",
      "*****************************************2********************************\n",
      "epoch[0]: loss: 14.365808 score: 0.000000\n",
      "Higher score: 0.000000\n",
      "epoch[1147]: loss: 3.134134 score: 0.030171\n",
      "Higher score: 0.030171\n",
      "epoch[1148]: loss: 3.547733 score: 0.054675\n",
      "Higher score: 0.054675\n",
      "epoch[1149]: loss: 3.621109 score: 0.068576\n",
      "Higher score: 0.068576\n",
      "epoch[1151]: loss: 3.092065 score: 0.071282\n",
      "Higher score: 0.071282\n",
      "epoch[1152]: loss: 3.019412 score: 0.090104\n",
      "Higher score: 0.090104\n",
      "epoch[1375]: loss: 2.763650 score: 0.169108\n",
      "Higher score: 0.169108\n",
      "epoch[1376]: loss: 3.222412 score: 0.215289\n",
      "Higher score: 0.215289\n",
      "early stopping!!!!\n",
      "*****************************************3********************************\n",
      "epoch[0]: loss: 14.751060 score: 0.000000\n",
      "Higher score: 0.000000\n",
      "epoch[275]: loss: 4.192016 score: 0.000060\n",
      "Higher score: 0.000060\n",
      "epoch[276]: loss: 4.141838 score: 0.000149\n",
      "Higher score: 0.000149\n",
      "epoch[279]: loss: 4.087384 score: 0.000400\n",
      "Higher score: 0.000400\n",
      "epoch[280]: loss: 4.111419 score: 0.001290\n",
      "Higher score: 0.001290\n",
      "epoch[281]: loss: 4.114763 score: 0.002617\n",
      "Higher score: 0.002617\n",
      "epoch[282]: loss: 4.100873 score: 0.004784\n",
      "Higher score: 0.004784\n",
      "epoch[283]: loss: 4.104281 score: 0.008037\n",
      "Higher score: 0.008037\n",
      "epoch[284]: loss: 4.138186 score: 0.011121\n",
      "Higher score: 0.011121\n",
      "epoch[285]: loss: 4.164279 score: 0.013894\n",
      "Higher score: 0.013894\n",
      "epoch[286]: loss: 4.144103 score: 0.016715\n",
      "Higher score: 0.016715\n",
      "epoch[287]: loss: 4.056291 score: 0.020087\n",
      "Higher score: 0.020087\n",
      "epoch[288]: loss: 4.124197 score: 0.022861\n",
      "Higher score: 0.022861\n",
      "epoch[289]: loss: 4.109845 score: 0.025885\n",
      "Higher score: 0.025885\n",
      "epoch[290]: loss: 4.132945 score: 0.028972\n",
      "Higher score: 0.028972\n",
      "epoch[291]: loss: 4.117362 score: 0.031988\n",
      "Higher score: 0.031988\n",
      "epoch[292]: loss: 4.016935 score: 0.036460\n",
      "Higher score: 0.036460\n",
      "epoch[293]: loss: 4.124887 score: 0.041386\n",
      "Higher score: 0.041386\n",
      "epoch[294]: loss: 4.030315 score: 0.046535\n",
      "Higher score: 0.046535\n",
      "epoch[295]: loss: 4.125464 score: 0.051867\n",
      "Higher score: 0.051867\n",
      "epoch[296]: loss: 4.013964 score: 0.059609\n",
      "Higher score: 0.059609\n",
      "epoch[297]: loss: 4.072126 score: 0.068378\n",
      "Higher score: 0.068378\n",
      "epoch[298]: loss: 4.090621 score: 0.078484\n",
      "Higher score: 0.078484\n",
      "epoch[299]: loss: 4.209837 score: 0.085292\n",
      "Higher score: 0.085292\n",
      "epoch[300]: loss: 4.026341 score: 0.094741\n",
      "Higher score: 0.094741\n",
      "epoch[301]: loss: 4.139811 score: 0.107052\n",
      "Higher score: 0.107052\n",
      "epoch[302]: loss: 4.175927 score: 0.113305\n",
      "Higher score: 0.113305\n",
      "epoch[303]: loss: 4.091607 score: 0.118739\n",
      "Higher score: 0.118739\n",
      "epoch[304]: loss: 4.001894 score: 0.128406\n",
      "Higher score: 0.128406\n",
      "epoch[305]: loss: 4.181044 score: 0.134244\n",
      "Higher score: 0.134244\n",
      "epoch[306]: loss: 3.998312 score: 0.145212\n",
      "Higher score: 0.145212\n",
      "epoch[307]: loss: 4.130105 score: 0.150758\n",
      "Higher score: 0.150758\n",
      "epoch[308]: loss: 4.035444 score: 0.155012\n",
      "Higher score: 0.155012\n",
      "epoch[309]: loss: 3.956907 score: 0.158693\n",
      "Higher score: 0.158693\n",
      "epoch[349]: loss: 4.052365 score: 0.169283\n",
      "Higher score: 0.169283\n",
      "epoch[350]: loss: 4.011215 score: 0.210938\n",
      "Higher score: 0.210938\n",
      "epoch[351]: loss: 3.867086 score: 0.226449\n",
      "Higher score: 0.226449\n",
      "epoch[352]: loss: 4.232284 score: 0.232782\n",
      "Higher score: 0.232782\n",
      "epoch[353]: loss: 3.944339 score: 0.236625\n",
      "Higher score: 0.236625\n",
      "epoch[355]: loss: 3.964715 score: 0.238963\n",
      "Higher score: 0.238963\n",
      "epoch[356]: loss: 3.790819 score: 0.251850\n",
      "Higher score: 0.251850\n",
      "epoch[357]: loss: 3.735840 score: 0.265827\n",
      "Higher score: 0.265827\n",
      "early stopping!!!!\n",
      "*****************************************4********************************\n",
      "epoch[0]: loss: 15.042837 score: 0.000000\n",
      "Higher score: 0.000000\n",
      "epoch[423]: loss: 4.405212 score: 0.022503\n",
      "Higher score: 0.022503\n",
      "epoch[424]: loss: 3.930989 score: 0.054861\n",
      "Higher score: 0.054861\n",
      "epoch[425]: loss: 3.917493 score: 0.057083\n",
      "Higher score: 0.057083\n",
      "epoch[440]: loss: 3.874059 score: 0.058337\n",
      "Higher score: 0.058337\n",
      "epoch[441]: loss: 3.912513 score: 0.058508\n",
      "Higher score: 0.058508\n",
      "epoch[442]: loss: 4.034527 score: 0.070029\n",
      "Higher score: 0.070029\n",
      "epoch[458]: loss: 4.384153 score: 0.082206\n",
      "Higher score: 0.082206\n",
      "epoch[459]: loss: 4.051320 score: 0.108670\n",
      "Higher score: 0.108670\n",
      "epoch[460]: loss: 4.050613 score: 0.117224\n",
      "Higher score: 0.117224\n",
      "epoch[472]: loss: 4.486599 score: 0.178508\n",
      "Higher score: 0.178508\n",
      "epoch[473]: loss: 4.046335 score: 0.242986\n",
      "Higher score: 0.242986\n",
      "epoch[474]: loss: 3.685552 score: 0.253381\n",
      "Higher score: 0.253381\n",
      "epoch[536]: loss: 3.647866 score: 0.284066\n",
      "Higher score: 0.284066\n",
      "epoch[537]: loss: 3.929022 score: 0.304316\n",
      "Higher score: 0.304316\n",
      "epoch[538]: loss: 3.978496 score: 0.309486\n",
      "Higher score: 0.309486\n",
      "epoch[539]: loss: 3.882368 score: 0.310260\n",
      "Higher score: 0.310260\n",
      "epoch[540]: loss: 3.756119 score: 0.317436\n",
      "Higher score: 0.317436\n",
      "epoch[550]: loss: 4.050080 score: 0.318042\n",
      "Higher score: 0.318042\n",
      "epoch[1469]: loss: 2.770354 score: 0.345154\n",
      "Higher score: 0.345154\n",
      "epoch[2440]: loss: 2.875761 score: 0.350521\n",
      "Higher score: 0.350521\n",
      "epoch[2441]: loss: 2.735927 score: 0.359711\n",
      "Higher score: 0.359711\n",
      "epoch[2442]: loss: 3.024416 score: 0.359958\n",
      "Higher score: 0.359958\n",
      "epoch[3738]: loss: 2.083955 score: 0.364669\n",
      "Higher score: 0.364669\n",
      "epoch[3739]: loss: 2.131988 score: 0.383520\n",
      "Higher score: 0.383520\n",
      "epoch[3740]: loss: 2.260331 score: 0.388548\n",
      "Higher score: 0.388548\n",
      "epoch[4118]: loss: 2.736478 score: 0.406169\n",
      "Higher score: 0.406169\n",
      "epoch[4119]: loss: 2.684853 score: 0.418362\n",
      "Higher score: 0.418362\n",
      "epoch[4120]: loss: 3.997167 score: 0.421655\n",
      "Higher score: 0.421655\n",
      "epoch[4333]: loss: 2.739676 score: 0.434404\n",
      "Higher score: 0.434404\n",
      "epoch[4334]: loss: 1.914044 score: 0.445475\n",
      "Higher score: 0.445475\n",
      "epoch[4335]: loss: 1.961488 score: 0.459306\n",
      "Higher score: 0.459306\n",
      "epoch[4336]: loss: 2.448033 score: 0.469863\n",
      "Higher score: 0.469863\n",
      "epoch[5220]: loss: 2.812140 score: 0.473424\n",
      "Higher score: 0.473424\n",
      "epoch[5221]: loss: 1.948529 score: 0.492429\n",
      "Higher score: 0.492429\n",
      "epoch[5222]: loss: 1.687989 score: 0.505165\n",
      "Higher score: 0.505165\n",
      "epoch[5223]: loss: 1.016563 score: 0.511750\n",
      "Higher score: 0.511750\n",
      "early stopping!!!!\n",
      "*****************************************5********************************\n",
      "epoch[0]: loss: 12.905913 score: 0.000000\n",
      "Higher score: 0.000000\n",
      "epoch[799]: loss: 2.326327 score: 0.001796\n",
      "Higher score: 0.001796\n",
      "epoch[800]: loss: 2.216952 score: 0.004531\n",
      "Higher score: 0.004531\n",
      "epoch[801]: loss: 1.886559 score: 0.005227\n",
      "Higher score: 0.005227\n",
      "epoch[802]: loss: 2.141435 score: 0.006017\n",
      "Higher score: 0.006017\n",
      "epoch[803]: loss: 2.025303 score: 0.007359\n",
      "Higher score: 0.007359\n",
      "epoch[804]: loss: 2.225095 score: 0.008456\n",
      "Higher score: 0.008456\n",
      "epoch[805]: loss: 2.300092 score: 0.009184\n",
      "Higher score: 0.009184\n",
      "epoch[806]: loss: 2.080224 score: 0.010227\n",
      "Higher score: 0.010227\n",
      "epoch[807]: loss: 2.109382 score: 0.011131\n",
      "Higher score: 0.011131\n",
      "epoch[808]: loss: 1.654960 score: 0.012065\n",
      "Higher score: 0.012065\n",
      "epoch[819]: loss: 2.120400 score: 0.013497\n",
      "Higher score: 0.013497\n",
      "epoch[820]: loss: 1.645741 score: 0.016578\n",
      "Higher score: 0.016578\n",
      "epoch[821]: loss: 2.204716 score: 0.018320\n",
      "Higher score: 0.018320\n",
      "epoch[822]: loss: 1.483213 score: 0.019643\n",
      "Higher score: 0.019643\n",
      "epoch[823]: loss: 2.032409 score: 0.020371\n",
      "Higher score: 0.020371\n",
      "epoch[824]: loss: 2.051796 score: 0.020948\n",
      "Higher score: 0.020948\n",
      "epoch[829]: loss: 2.445220 score: 0.023668\n",
      "Higher score: 0.023668\n",
      "epoch[830]: loss: 1.872009 score: 0.026042\n",
      "Higher score: 0.026042\n",
      "epoch[831]: loss: 2.079888 score: 0.027101\n",
      "Higher score: 0.027101\n",
      "epoch[883]: loss: 2.062562 score: 0.027263\n",
      "Higher score: 0.027263\n",
      "epoch[884]: loss: 2.043191 score: 0.029641\n",
      "Higher score: 0.029641\n",
      "epoch[885]: loss: 2.102852 score: 0.031482\n",
      "Higher score: 0.031482\n",
      "epoch[886]: loss: 2.039209 score: 0.033206\n",
      "Higher score: 0.033206\n",
      "epoch[887]: loss: 1.936483 score: 0.035723\n",
      "Higher score: 0.035723\n",
      "epoch[888]: loss: 2.071491 score: 0.037256\n",
      "Higher score: 0.037256\n",
      "epoch[889]: loss: 1.964587 score: 0.039028\n",
      "Higher score: 0.039028\n",
      "epoch[890]: loss: 1.730953 score: 0.039901\n",
      "Higher score: 0.039901\n",
      "epoch[891]: loss: 1.969688 score: 0.040081\n",
      "Higher score: 0.040081\n",
      "epoch[900]: loss: 1.862891 score: 0.040275\n",
      "Higher score: 0.040275\n",
      "epoch[901]: loss: 1.787382 score: 0.042360\n",
      "Higher score: 0.042360\n",
      "epoch[902]: loss: 2.137887 score: 0.042528\n",
      "Higher score: 0.042528\n",
      "epoch[903]: loss: 1.859344 score: 0.042983\n",
      "Higher score: 0.042983\n",
      "epoch[904]: loss: 2.038995 score: 0.043130\n",
      "Higher score: 0.043130\n",
      "epoch[907]: loss: 1.865605 score: 0.044142\n",
      "Higher score: 0.044142\n",
      "epoch[908]: loss: 2.408582 score: 0.044558\n",
      "Higher score: 0.044558\n",
      "epoch[947]: loss: 1.932534 score: 0.047964\n",
      "Higher score: 0.047964\n",
      "epoch[948]: loss: 2.005594 score: 0.050852\n",
      "Higher score: 0.050852\n",
      "epoch[949]: loss: 1.650545 score: 0.053290\n",
      "Higher score: 0.053290\n",
      "epoch[950]: loss: 2.095293 score: 0.054915\n",
      "Higher score: 0.054915\n",
      "epoch[951]: loss: 1.892874 score: 0.055852\n",
      "Higher score: 0.055852\n",
      "epoch[952]: loss: 1.611816 score: 0.056678\n",
      "Higher score: 0.056678\n",
      "epoch[953]: loss: 2.203635 score: 0.056873\n",
      "Higher score: 0.056873\n",
      "epoch[1002]: loss: 1.666542 score: 0.058511\n",
      "Higher score: 0.058511\n",
      "epoch[1003]: loss: 1.544027 score: 0.064060\n",
      "Higher score: 0.064060\n",
      "epoch[1004]: loss: 1.981895 score: 0.067497\n",
      "Higher score: 0.067497\n",
      "epoch[1005]: loss: 1.598141 score: 0.068776\n",
      "Higher score: 0.068776\n",
      "epoch[1006]: loss: 2.010921 score: 0.069381\n",
      "Higher score: 0.069381\n",
      "epoch[1007]: loss: 2.259824 score: 0.070019\n",
      "Higher score: 0.070019\n",
      "epoch[1008]: loss: 2.121027 score: 0.070691\n",
      "Higher score: 0.070691\n",
      "epoch[1021]: loss: 1.779923 score: 0.072018\n",
      "Higher score: 0.072018\n",
      "epoch[1022]: loss: 1.922485 score: 0.073837\n",
      "Higher score: 0.073837\n",
      "epoch[1023]: loss: 1.631342 score: 0.074639\n",
      "Higher score: 0.074639\n",
      "epoch[1024]: loss: 1.597078 score: 0.076483\n",
      "Higher score: 0.076483\n",
      "epoch[1025]: loss: 1.491877 score: 0.078693\n",
      "Higher score: 0.078693\n",
      "epoch[1026]: loss: 1.980612 score: 0.079052\n",
      "Higher score: 0.079052\n",
      "epoch[1044]: loss: 1.780014 score: 0.080071\n",
      "Higher score: 0.080071\n",
      "epoch[1045]: loss: 1.794852 score: 0.084937\n",
      "Higher score: 0.084937\n",
      "epoch[1046]: loss: 1.874144 score: 0.087663\n",
      "Higher score: 0.087663\n",
      "epoch[1047]: loss: 2.300341 score: 0.087687\n",
      "Higher score: 0.087687\n",
      "epoch[1053]: loss: 1.827807 score: 0.088925\n",
      "Higher score: 0.088925\n",
      "epoch[1054]: loss: 1.940463 score: 0.093604\n",
      "Higher score: 0.093604\n",
      "epoch[1055]: loss: 1.768736 score: 0.099742\n",
      "Higher score: 0.099742\n",
      "epoch[1056]: loss: 2.086957 score: 0.104731\n",
      "Higher score: 0.104731\n",
      "epoch[1057]: loss: 1.893224 score: 0.107845\n",
      "Higher score: 0.107845\n",
      "epoch[1058]: loss: 1.405803 score: 0.109063\n",
      "Higher score: 0.109063\n",
      "epoch[1290]: loss: 2.020931 score: 0.112984\n",
      "Higher score: 0.112984\n",
      "epoch[1291]: loss: 2.102900 score: 0.117168\n",
      "Higher score: 0.117168\n",
      "epoch[1292]: loss: 1.776166 score: 0.118623\n",
      "Higher score: 0.118623\n",
      "epoch[1293]: loss: 1.715499 score: 0.119347\n",
      "Higher score: 0.119347\n",
      "epoch[1294]: loss: 1.850970 score: 0.119770\n",
      "Higher score: 0.119770\n",
      "epoch[2049]: loss: 1.322010 score: 0.120966\n",
      "Higher score: 0.120966\n",
      "epoch[2050]: loss: 1.228374 score: 0.124508\n",
      "Higher score: 0.124508\n",
      "epoch[2076]: loss: 1.168310 score: 0.125422\n",
      "Higher score: 0.125422\n",
      "epoch[2077]: loss: 1.811250 score: 0.126225\n",
      "Higher score: 0.126225\n",
      "epoch[2336]: loss: 1.130707 score: 0.128035\n",
      "Higher score: 0.128035\n",
      "epoch[2337]: loss: 1.192367 score: 0.130294\n",
      "Higher score: 0.130294\n",
      "epoch[2338]: loss: 1.421710 score: 0.131192\n",
      "Higher score: 0.131192\n",
      "epoch[2339]: loss: 1.622294 score: 0.131685\n",
      "Higher score: 0.131685\n",
      "epoch[2353]: loss: 0.942767 score: 0.136259\n",
      "Higher score: 0.136259\n",
      "epoch[2354]: loss: 1.848810 score: 0.141604\n",
      "Higher score: 0.141604\n",
      "epoch[2355]: loss: 1.472699 score: 0.145578\n",
      "Higher score: 0.145578\n",
      "epoch[2449]: loss: 1.470302 score: 0.146429\n",
      "Higher score: 0.146429\n",
      "epoch[2540]: loss: 1.292406 score: 0.148001\n",
      "Higher score: 0.148001\n",
      "epoch[2541]: loss: 1.432674 score: 0.153085\n",
      "Higher score: 0.153085\n",
      "epoch[2542]: loss: 1.072133 score: 0.156929\n",
      "Higher score: 0.156929\n",
      "epoch[2543]: loss: 1.885902 score: 0.159953\n",
      "Higher score: 0.159953\n",
      "epoch[2596]: loss: 1.437437 score: 0.160475\n",
      "Higher score: 0.160475\n",
      "epoch[2597]: loss: 1.435343 score: 0.161733\n",
      "Higher score: 0.161733\n",
      "epoch[3360]: loss: 1.162449 score: 0.162614\n",
      "Higher score: 0.162614\n",
      "epoch[3361]: loss: 1.095595 score: 0.162946\n",
      "Higher score: 0.162946\n",
      "epoch[3381]: loss: 1.324489 score: 0.164629\n",
      "Higher score: 0.164629\n",
      "epoch[3382]: loss: 1.087949 score: 0.168558\n",
      "Higher score: 0.168558\n",
      "epoch[3383]: loss: 0.865883 score: 0.169844\n",
      "Higher score: 0.169844\n",
      "epoch[3384]: loss: 1.370011 score: 0.170078\n",
      "Higher score: 0.170078\n",
      "epoch[3385]: loss: 1.149411 score: 0.170399\n",
      "Higher score: 0.170399\n",
      "epoch[3396]: loss: 1.016467 score: 0.170432\n",
      "Higher score: 0.170432\n",
      "epoch[3397]: loss: 1.054851 score: 0.172396\n",
      "Higher score: 0.172396\n",
      "epoch[3398]: loss: 1.381715 score: 0.173585\n",
      "Higher score: 0.173585\n",
      "epoch[3399]: loss: 1.354605 score: 0.174745\n",
      "Higher score: 0.174745\n",
      "epoch[3628]: loss: 0.880435 score: 0.175607\n",
      "Higher score: 0.175607\n",
      "epoch[3629]: loss: 1.418311 score: 0.177140\n",
      "Higher score: 0.177140\n",
      "epoch[3630]: loss: 1.047570 score: 0.178188\n",
      "Higher score: 0.178188\n",
      "epoch[3786]: loss: 1.049988 score: 0.179802\n",
      "Higher score: 0.179802\n",
      "epoch[3787]: loss: 0.929011 score: 0.182704\n",
      "Higher score: 0.182704\n",
      "epoch[3788]: loss: 1.181661 score: 0.185476\n",
      "Higher score: 0.185476\n",
      "epoch[3789]: loss: 1.151295 score: 0.186863\n",
      "Higher score: 0.186863\n",
      "epoch[3790]: loss: 1.023321 score: 0.187447\n",
      "Higher score: 0.187447\n",
      "epoch[4237]: loss: 1.109998 score: 0.190153\n",
      "Higher score: 0.190153\n",
      "epoch[4238]: loss: 0.741800 score: 0.192417\n",
      "Higher score: 0.192417\n",
      "epoch[4239]: loss: 0.982538 score: 0.194069\n",
      "Higher score: 0.194069\n",
      "epoch[4240]: loss: 0.905809 score: 0.196624\n",
      "Higher score: 0.196624\n",
      "epoch[4241]: loss: 0.783199 score: 0.198879\n",
      "Higher score: 0.198879\n",
      "epoch[4242]: loss: 1.677170 score: 0.200422\n",
      "Higher score: 0.200422\n",
      "epoch[4243]: loss: 0.731564 score: 0.201136\n",
      "Higher score: 0.201136\n",
      "epoch[4941]: loss: 0.856324 score: 0.201460\n",
      "Higher score: 0.201460\n",
      "epoch[5004]: loss: 1.171149 score: 0.203021\n",
      "Higher score: 0.203021\n",
      "epoch[5005]: loss: 0.726069 score: 0.203351\n",
      "Higher score: 0.203351\n",
      "epoch[5279]: loss: 0.553969 score: 0.204933\n",
      "Higher score: 0.204933\n",
      "epoch[5280]: loss: 1.051934 score: 0.205338\n",
      "Higher score: 0.205338\n",
      "epoch[5699]: loss: 0.819654 score: 0.206134\n",
      "Higher score: 0.206134\n",
      "epoch[5700]: loss: 1.155461 score: 0.206932\n",
      "Higher score: 0.206932\n",
      "epoch[5701]: loss: 0.700323 score: 0.207204\n",
      "Higher score: 0.207204\n",
      "epoch[5737]: loss: 0.747195 score: 0.207783\n",
      "Higher score: 0.207783\n",
      "epoch[6050]: loss: 0.749556 score: 0.208054\n",
      "Higher score: 0.208054\n",
      "epoch[6051]: loss: 0.728477 score: 0.210899\n",
      "Higher score: 0.210899\n",
      "epoch[6052]: loss: 1.111135 score: 0.213364\n",
      "Higher score: 0.213364\n",
      "epoch[6053]: loss: 1.056820 score: 0.214764\n",
      "Higher score: 0.214764\n",
      "epoch[6054]: loss: 1.366120 score: 0.215788\n",
      "Higher score: 0.215788\n",
      "epoch[6055]: loss: 0.796921 score: 0.216121\n",
      "Higher score: 0.216121\n",
      "epoch[6058]: loss: 1.050609 score: 0.216222\n",
      "Higher score: 0.216222\n",
      "epoch[6798]: loss: 0.726472 score: 0.218194\n",
      "Higher score: 0.218194\n",
      "epoch[6799]: loss: 0.794272 score: 0.218947\n",
      "Higher score: 0.218947\n",
      "epoch[6800]: loss: 0.782988 score: 0.219276\n",
      "Higher score: 0.219276\n",
      "epoch[6801]: loss: 1.028617 score: 0.219645\n",
      "Higher score: 0.219645\n",
      "epoch[7407]: loss: 0.823610 score: 0.219814\n",
      "Higher score: 0.219814\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\lab106\\xibo\\反向设计\\try15\\minRL.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lab106/xibo/%E5%8F%8D%E5%90%91%E8%AE%BE%E8%AE%A1/try15/minRL.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lab106/xibo/%E5%8F%8D%E5%90%91%E8%AE%BE%E8%AE%A1/try15/minRL.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/lab106/xibo/%E5%8F%8D%E5%90%91%E8%AE%BE%E8%AE%A1/try15/minRL.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lab106/xibo/%E5%8F%8D%E5%90%91%E8%AE%BE%E8%AE%A1/try15/minRL.ipynb#X20sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m score \u001b[39m=\u001b[39m test(val_loader, model, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lab106/xibo/%E5%8F%8D%E5%90%91%E8%AE%BE%E8%AE%A1/try15/minRL.ipynb#X20sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torch\\optim\\adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    107\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 108\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    109\u001b[0m            grads,\n\u001b[0;32m    110\u001b[0m            exp_avgs,\n\u001b[0;32m    111\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    112\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    113\u001b[0m            state_steps,\n\u001b[0;32m    114\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    115\u001b[0m            beta1,\n\u001b[0;32m    116\u001b[0m            beta2,\n\u001b[0;32m    117\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    118\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    119\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m    120\u001b[0m            )\n\u001b[0;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\torch\\optim\\functional.py:86\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     83\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m     85\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m     87\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[0;32m     89\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 新改模型定义\n",
    "folds = 5\n",
    "seed = 2020\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "kf_scores = []\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X1)):\n",
    "    print('*****************************************{}********************************'.format(i + 1))\n",
    "    model = TransformerModel(ntoken=1, d_model=5, nhead=5, d_hid=16, max_len1=max_seq_len1, max_len2=max_seq_len2, nlayers=5 , dropout=0.2)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    X1_train, X2_train, y_train = X1[train_index], X2[train_index], Y[train_index]\n",
    "    X1_val, X2_val, y_val = X1[valid_index], X2[valid_index], Y[valid_index]\n",
    "\n",
    "    src_key_padding_mask1_train, src_key_padding_mask2_train = src_key_padding_mask1[train_index], src_key_padding_mask2[train_index]\n",
    "    src_key_padding_mask1_val, src_key_padding_mask2_val = src_key_padding_mask1[valid_index], src_key_padding_mask2[valid_index]\n",
    "    \n",
    "    train_loader = DataLoader(dataset = MaterialDataset(X1_train, X2_train, y_train, src_key_padding_mask1_train, src_key_padding_mask2_train), batch_size=1000, shuffle=False, num_workers=0)\n",
    "    val_loader = DataLoader(dataset = MaterialDataset(X1_val, X2_val, y_val, src_key_padding_mask1_val, src_key_padding_mask2_val), batch_size=1000, shuffle=False, num_workers=0)\n",
    "\n",
    "    max_score = -1\n",
    "    best_epoch = 0\n",
    "    model.train()\n",
    "    for epoch in range(15000):\n",
    "        if epoch - best_epoch > 2000:\n",
    "            print('early stopping!!!!')\n",
    "            break\n",
    "        for j, (x1, x2, y, padding_mask1, padding_mask2) in enumerate(train_loader):\n",
    "            x1 = x1.float().to(device)\n",
    "            x2 = x2.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            padding_mask1 = padding_mask1.float().to(device)\n",
    "            padding_mask2 = padding_mask2.float().to(device)\n",
    "\n",
    "            pre_y = model(x1.permute(2,0,1), x2.permute(2,0,1), padding_mask1, padding_mask2)\n",
    "\n",
    "            loss = criterion(pre_y, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        score = test(val_loader, model, device)\n",
    "        model.train()\n",
    "\n",
    "        # print(f\"epoch[{epoch}]: loss: {loss:>7f} score: {score:>7f}\")\n",
    "        \n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_epoch = epoch\n",
    "            print(f\"epoch[{epoch}]: loss: {loss:>7f} score: {score:>7f}\")\n",
    "            print(f\"Higher score: {(max_score):>4f}\")\n",
    "            torch.save(model, f'minRL_{i}.pt')\n",
    "    kf_scores.append(max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28916521833402\n",
      "[0.16379419329971, 0.21528940817405273, 0.2658270515948582, 0.5117502202674589]\n",
      "0.13347685413340304\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(kf_scores))\n",
    "print(kf_scores)\n",
    "print(np.std(kf_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minRL new_27_limit_5\n",
    "# seed == 2020\n",
    "0.4009413858445585\n",
    "[0.34387797551922605, 0.5383718305817475, 0.3205743514327021]\n",
    "0.0976425797808438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minRL new_27_limit_3\n",
    "# seed == 2020\n",
    "0.3958051487992849\n",
    "[0.48662754341797954, 0.5688517562348477, 0.13193614674502763]\n",
    "0.18957908284261504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minRL new_27\n",
    "# seed == 2020\n",
    "0.23430058456247915\n",
    "[0.012273413554796608, 0.5903697701056589, 0.10025857002698191]\n",
    "0.25432825981382967"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11ca1b389f0f7ff512e0d1900cffe853c843f5d9714fc38dfd5c75c4c45d4a2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
